{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Environmental Science Day 6\n",
    "## Topics\n",
    "* Pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "Foremost, I have to inform you that [pandas](https://www.quora.com/Why-is-the-pandas-Python-library-named-pandas) has nothing to do with [pandas](https://en.wikipedia.org/wiki/Giant_panda). Sorry about that. Secondly, I have to tell you that pandas can be a bit confusing at times and I still haven't figured out everything myself. However, I will give my best to teach you the things I know and those I should know about pandas. \n",
    "\n",
    "This notebook might take a bit longer then the days before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is pandas?\n",
    "You know [spreadsheets](https://en.wikipedia.org/wiki/Spreadsheet), right? \n",
    "\n",
    "<img src=\"https://i.pinimg.com/originals/04/5b/91/045b9193b35a53ea001963032614e599.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/ align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Basically pandas does the same, but a bit more abstract and way better. Today you will learn how to read data into pandas and work with it in meaningful ways. When you import pandas it is convention to do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, take a look at [this video](https://youtu.be/CmorAWRsCAw) to get a introduction to pandas and why it is useful.\n",
    "\n",
    "### Practice Questions\n",
    "* What is the advantage of pandas of regular spreadsheets like excel?\n",
    "* What is data wrangling?\n",
    "\n",
    "In the following sections we will use the well known [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) to explore pandas a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not worry if those lines confuse you. \n",
    "# They are only needed to get some data to work with and you do not need to understand them.\n",
    "# If you are curious take a look here:\n",
    "# https://stackoverflow.com/questions/38105539/how-to-convert-a-scikit-learn-dataset-to-a-pandas-dataset\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "def load_iris():\n",
    "    \"\"\"Loads the iris dataset and returns it as a dataframe\"\"\"\n",
    "    iris = datasets.load_iris()\n",
    "    iris_df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                         columns= iris['feature_names'] + ['target'])\n",
    "    return iris_df\n",
    "\n",
    "iris_df = load_iris()\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it looks pretty similar to excel at first glance.\n",
    "\n",
    "After watching the video try to calculate the median of the column \"sepal width (cm)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rows and Columns\n",
    "As I always confuse them, here is a little reminder on rows and columns:\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-2c10ea6a9bdf9c6ae2c9031fb15c6723\" alt=\"Drawing\" style=\"width: 250px;\"/ align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Basics\n",
    "Take a look at [this video](https://youtu.be/F6kmIpWWEdU) to get a basic overview of the dataframe data strucure. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Questions\n",
    "* What is the \"shape\" of a dataframe?\n",
    "* How can you get the column names of a dataframe?\n",
    "* What is the difference between a series and a dataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Selection in Dataframes\n",
    "When you have collected and cleaned your data you want to do something with it. The first thing is usually looking at it. So lets take a look at a few ways to access your dataframe.\n",
    "* Access a certain subset of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simply use slicing, as you already learned with lists\n",
    "iris_df[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Access a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simply use the name in square brackets\n",
    "# The .head() function is not neccesary for this, but it is nicer to look at. \n",
    "# Try the line without it. \n",
    "iris_df[\"sepal width (cm)\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing we got back from pandas when we asked for a certain column was a series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(iris_df[\"sepal width (cm)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ought not confuse you. [Simply put](https://stackoverflow.com/questions/26047209/what-is-the-difference-between-a-pandas-series-and-a-single-column-dataframe) \"a series is a datastructure for a single column of a dataframe\" and a dataframe \"can be thought of as a dict-like container for series objects\".\n",
    "\n",
    "* Access several columns at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simply create a list of the columns you want to access\n",
    "cols = [\"sepal width (cm)\", \"sepal length (cm)\"]\n",
    "iris_df[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Calculations\n",
    "As the dataframe contains data you want to work with, the most powerful property of pandas is the easy usage of functions to calculate new informations. The most useful to get a quick overview for a dataframe is .describe(), which will give you the most common describers of data like mean or max values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do this for specific describers and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_df[\"sepal width (cm)\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Indexing\n",
    "Often you want to filter for a specific value. For example you only want to get all the rows where the sepal width is above 4. This is integrated in pandas by the so called boolean indexing. For this you have to access the dataframe as usual, but instead of typing the column you give it a boolean expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_df[iris_df[\"sepal width (cm)\"] > 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine boolean indexing with regular a regular access on certain columns. So in the following case you would get the petal width for all flowers were the sepal width is above 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_df[\"petal width (cm)\"][iris_df[\"sepal width (cm)\"] > 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Questions\n",
    "* When is it impossible to access a column with the statement df.columnname, but you have to type df[columnname] instead?\n",
    "* What is the index of a dataframe and what types can it have?\n",
    "* What is the keyword \"inplace\" used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Use the code provided above to read in the iris dataset and do the following:\n",
    "* Get the minimal value for the column 'petal length'\n",
    "* Select all rows where target is 0 and use the describe function on this\n",
    "* Create a slice of the last two rows of the dataframe\n",
    "\n",
    "<img src=\"https://memegenerator.net/img/instances/73988552/pythonpandas-is-easy-just-do-such-and-such.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/ align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataframes from Data\n",
    "Pandas allows you to create dataframes in very different ways. But we can savely assume that most of the time you will read in .csv files, as they are the most common format for raw data. First let us create a csv file we can play around with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Again, you do not need to get this code. But it will not hurt if you try.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "def create_csv():\n",
    "    \"\"\"Creates a dataframe with random data and saves it as a csv\"\"\"\n",
    "    # Create a dataframe with only NaN\n",
    "    df = pd.DataFrame(np.nan, index=range(50), columns=range(4))\n",
    "    # Fill the columns\n",
    "    for i, column in enumerate(df.columns):\n",
    "        # Make the first column retain some NaN\n",
    "        if i == 0:\n",
    "            fill_till = random.randint(10, random.randint(10, 51))\n",
    "            df.loc[:fill_till-1,column] = np.random.randint(low=random.randrange(100),\n",
    "                                                            high=random.randrange(100, 1000),\n",
    "                                                            size=fill_till)\n",
    "        # Fill all others completely\n",
    "        else:\n",
    "            df.loc[:,column] = np.random.randint(low=random.randrange(100),\n",
    "                                                 high=random.randrange(100, 1000),\n",
    "                                                 size=50)\n",
    "    # Replace the NaN\n",
    "    df.fillna(value=-999, inplace=True)\n",
    "    # Give the columns new names\n",
    "    df.columns = [\"Probe1\", \"Probe2\", \"Probe3\", \"Probe4\"]    \n",
    "    # Create a datetime index\n",
    "    times = pd.DatetimeIndex(freq='2s', start=datetime.datetime.now(), periods=50)\n",
    "    df.index = times\n",
    "    df.index.name = \"Datetime\"\n",
    "    # Write to a file\n",
    "    df.to_csv(\"probe_measurements.csv\", sep=\";\")            \n",
    "            \n",
    "create_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this code and in your directory a file named \"probe_measurements.csv\" should pop up. Now we want to read it in again. We have to make sure we get the seperator right and find a way to handle the NaNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_in_csv(file):\n",
    "    \"\"\"Reads in the file we just created\"\"\"\n",
    "    df = pd.read_csv(file, sep=\";\", header=0, index_col=0, na_values=-999)\n",
    "    return df\n",
    "\n",
    "df = read_in_csv(\"probe_measurements.csv\")\n",
    "df.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas method read_csv() has [quite a lot](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) of additional keywords to play around with. For our easy example we only needed four, but a bit more untidy data might force you to play around a lot more. For a little more information on how to create dataframes take a look [here](https://youtu.be/3k0HbcUGErE).\n",
    "\n",
    "### Practice Questions\n",
    "* What would happen if we would not define **na_values=-999**?\n",
    "* What does df.tail() do?\n",
    "* What kinds of Dataframe creation methods exist beside read_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Download [this file](http://unstats.un.org/unsd/environment/excel_file_tables/2013/Energy%20Indicators.xls), which is provided by UN and contains energy indicators for 2013. Your task is to create a dataframe called \"energy\" that contains all entries in the right format. You should consider the following:\n",
    "* This is an Excel file, not a csv (If this confuses you a lot take a look at [this video](https://youtu.be/-0NwrcZOKhQ?t=16m))\n",
    "* The upper part is unecessary and you should not read it in (do not delete parts of the file)\n",
    "* The first two columns contain no information\n",
    "* Make sure all missing data is saved as NaN\n",
    "* There are several countries with numbers and/or parenthesis in their name. Be sure to remove these (you can access the string values of a column by column.str).\n",
    "    * 'Bolivia (Plurinational State of)' should be 'Bolivia'\n",
    "    * 'Switzerland17' should be 'Switzerland'\n",
    "    \n",
    "Finally, let pandas describe the dataset for you.\n",
    "    \n",
    "Hint: This exercise will be a bit challenging. Do not lose heart if this takes some time to sort out. \n",
    "\n",
    "Hint: If you produce more than ten lines of code you are probably doing something wrong.\n",
    "\n",
    "Inspired by \"Introduction to Data Science\" MOOC by the University of Michigan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataframes as Files\n",
    "As stated above, you will mainly need to know how to save dataframes as csv files. First take a look at [this video](https://youtu.be/-0NwrcZOKhQ?t=11m49s) (you can stop at 15.30 Min). As you see writing to csv is not very complicated. The main syntax is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"name.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as the method to read_csv(), to_csv() has [quite a lot of keyword arguments](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html). I will not go into detail here, as most are for edge cases, where you require a certain format. \n",
    "\n",
    "You can also save files as excel type, but this is a bit more complicated if you have to use several sheets.\n",
    "\n",
    "### Practice Questions\n",
    "* Which kinds of files can pandas create in addition to csv and excel?\n",
    "\n",
    "### Exercise 3\n",
    "Write the energy dataframe you created in the last exercise to a csv file in a more easily readable format than originally provided. Test if is easily readable with e.g. Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Selection in Dataframes\n",
    "We already learned how to access rows or columns and now we learn to access both rows and columns at once. Pandas provides three main methods for this:\n",
    "* .loc[ ]  is for selecting the rows by the index and the columns by their names.\n",
    "* .iloc[ ]  is for selecting the rows and columns by their integer position (i for integer)\n",
    "* .ix[ ] mixes the two above by allowing you using labels and integers together\n",
    "\n",
    "For an overview take a look at [this video](https://youtu.be/xvpNA7bC8cs). Let's try the different methods step by step. \n",
    " \n",
    "### .loc[ ] Examples\n",
    "So the basic syntax is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[row_index_labels, column_name_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for example we want to get the first ten rows and the data from Probe 1 and Probe 2 from our dataframe created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = df.index\n",
    "first_ten = index[:10]\n",
    "print(df.loc[first_ten, [\"Probe1\", \"Probe2\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could want to get all rows for Probe 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df.loc[:, \"Probe1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you try. Use .loc[ ] to get rows 30 to 40 for all Probes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use .loc[ ] with boolean expressions, which comes in handy sometimes. You use it in the same way as before. Just tell pandas the rows you want and then the columns. Simply switch the labels with boolean expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df.loc[df[\"Probe2\"] > 500, \"Probe3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Questions\n",
    "* Describe the rules of the output of the statement above with your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .iloc[ ] Examples\n",
    "Basic syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.iloc[numbers_of_rows, numbers_of_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we want to get rows 15 to 25 and the columns in positon 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df.iloc[15: 25, [2, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .ix[ ] Examples\n",
    "As stated above .ix[ ] allows you to combine what we have learned so far with iloc and loc. So for example you want to access the first ten rows and the column of Probe 3. You should not get to used to this way of indexing as it will soon be [removed from pandas](https://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated), but you should not it exists as you might see it in other peoples code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.ix[:10, \"Probe3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Read in the energy.csv you created in the last exercise and do the following using loc or iloc (and some other things we learned so far):\n",
    "* Display the first five countries with all columns.\n",
    "* Display all countries that begin with \"A\" and calculate the mean values for all columns.\n",
    "* Display the renewable energy production of the countries 50 to 100 in ascending order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copies and Views\n",
    "When using all those different ways to access parts of dataframes confusion can arise, as some methods create copies (you get a new dataframe) and some other create a view (you still work with the old dataframe, but only see a part of it). However [general rules](https://stackoverflow.com/questions/23296282/what-rules-does-pandas-use-to-generate-a-view-vs-a-copy) exist. For convenience I made a 'copy':\n",
    "\n",
    "\"\n",
    "\n",
    "Here are the rules, subsequent override:\n",
    "* All operations generate a copy\n",
    "* If **inplace=True** is provided, it will modify in-place; only some operations support this\n",
    "* An indexer that sets, e.g. **.loc/.ix/.iloc/.iat/.at** will set inplace.\n",
    "* An indexer that gets on a single-dtyped object is almost always a view (depending on the memory layout it may not be that's why this is not reliable). This is mainly for efficiency. (the example from above is for **.query**; this will always return a copy as its evaluated by **numexpr**)\n",
    "* An indexer that gets on a multiple-dtyped object is always a copy.\n",
    "\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Questions\n",
    "* Is there a 'best' way to access a dataframe?\n",
    "* What is the difference between a copy and a view?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping Dataframes\n",
    "Looking at data often requires you to make a selection, as you only want to look at certain cases and not the whole dataset. For example in the energy dataset we used it might be interesting to look at the data grouped by continent. Pandas provides a very useful tool for this: groupby(). Take a look at [this video](https://youtu.be/Wb2Tp35dZ-I) to get an introduction. As the dataframes we have used so far do not have any categories to group them by we will take a look at [another dataset](https://www.kaggle.com/abcsds/pokemon). (You will have to create a Kaggle account to download it, if you do not want to use your own mail adress for this, try [this](https://www.10minutemail.com) disposable one). Now, lets read it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# You have to download the pokemon.csv to the same directory as this notebook\n",
    "pokemon = pd.read_csv(\"pokemon.csv\")\n",
    "# Include=\"all\" so categorical data gets included\n",
    "pokemon.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the video groupby() creates a key and a value after your specifications. The keys are the categories you defined and the values is the data that relates to that key. So for example if you group our dataframe by the 'Type 1' your Keys will be Grass, Fire, Water and so forth and your values will be all pokemon that fall under this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key, value in pokemon.groupby(\"Type 1\"):\n",
    "    print(\"\\n\\n\" + key)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groupby() is also quite handy if you want to calculate metrics of your your groups. The fancy part here is that you do not have to access all groups seperately. groupby() objects directly allow to call the metrics on themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_object = pokemon.groupby(\"Type 1\")\n",
    "groupby_object.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we calculated the mean for all columns. Obviously taking the mean of such things as the legendary status of a pokemon does not make much sense. Therefore, we can also apply this only to the columns we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_object[[\"Attack\", \"Defense\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a little teaser for tomorrow, pandas can also create figures relatively easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next line is needed to create the figure in this notebook\n",
    "%matplotlib inline\n",
    "groupby_object[\"Speed\"].mean().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Questions\n",
    "* In what form does groupby() return its results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "Use the pokemon dataframe to do the following:\n",
    "* Use groupby to print all the first 10 individual pokemon names belonging to teach 'Type 1'\n",
    "* Group the dataframe by Generation and save results in a dictionary, with the generation as key and the dataframe of the generation as value\n",
    "* Print the amount of pokemons in each Type 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Dataframes (concat and merge)\n",
    "Often you come across that you have several dataframes that contain data that relates to each other. Now pandas allows you combine those dataframes with two main methods:\n",
    "* [concat](https://youtu.be/WGOEFok1szA): mainly used to combine dataframes simply by adding them along the rows or columns\n",
    "* [merge](https://youtu.be/h4hOPGo4UVU): mainly used to combine dataframes in such a way that only certain parts of them are combined. There are four main kinds of merges (often also called joins)\n",
    "\n",
    "<img src=\"http://www.datasciencemadesimple.com/wp-content/uploads/2017/09/join-or-merge-in-python-pandas-1.png\n",
    "\" alt=\"Drawing\" style=\"width: 400px;\"/ align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to break it down, one could say that concat is for combining complete dataframes, wihout changing them, while merge allows you to specify how they should be combined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat\n",
    "Here is a little example. We have two pandas series. One containig the attack values of all pokemon and one containing all the defense values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_attack = pokemon[\"Attack\"]\n",
    "pokemon_defense = pokemon[\"Defense\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to combine those two into one dataframe using concat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_def = pd.concat([pokemon_attack, pokemon_defense])\n",
    "att_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we now have a series with 1600 instead of 800 entries. But this is not what we wanted isn't it? We want two columns with 800 entries each. The problem we stumbled across is that concat can either combine via rows or columns. As the default is along the rows and we did not specify otherwise, it did just that. So if we want to concat along the columns we have to tell pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_def = pd.concat([pokemon_attack, pokemon_defense], axis=1)\n",
    "att_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates the dataframe we wanted to get!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge\n",
    "First let us create two dataframes, each containing parts of the pokemon dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pok_df1 = pokemon.iloc[:12,:3]\n",
    "pok_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pok_df2 = pokemon.iloc[5:10,3:]\n",
    "pok_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us explore how different types of merges give us different results. As the only thing both dataframes have in common is the index, we have to merge using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(pok_df1, pok_df2, how=\"inner\", left_index=True, right_index=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did a inner join, only those entries entries are in the dataframe that existed in both dataframes. In this case this equals to the second dataframe. Now lets look at an outer merge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(pok_df1, pok_df2, how=\"outer\", left_index=True, right_index=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get a larger dataframe, but have many 'NaN' as pandas has no information to fill the gaps with. \n",
    "\n",
    "Here is an [additional article](http://www.datasciencemadesimple.com/join-merge-data-frames-pandas-python/) on merging if you want to get a bit more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Questions\n",
    "* What is the difference between concat and merge?\n",
    "* What is the difference between an outer and an inner join?\n",
    "* What are the consequences of using an outer or an inner join?\n",
    "* What does the axis keyword argument specify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "The following codes produces a list of several dataframes. Concatenate them once along the rows and once along the columns. Which one makes more sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def create_dfs():\n",
    "    \"\"\"Creates several small dataframes and returns them in a list\"\"\"\n",
    "    df_list = []\n",
    "    for i in range(3):\n",
    "        df = pd.Dataframe({\n",
    "            \"probe\": [1, 2, 3],\n",
    "            \"humidity\": [random.random(), random.random(), random.random()],\n",
    "            \"temperature\": [random.randint(-100,100), random.randint(-100,100), random.randint(-100,100)]\n",
    "        })\n",
    "        df_list.append(df)\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "Merge the following two dataframes in a meaningful way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\n",
    "            \"probe\": [1, 2, 3],\n",
    "            \"humidity\": [random.random(), random.random(), random.random()],\n",
    "        })\n",
    "df2 = pd.DataFrame({\n",
    "            \"probe\": [1, 2, 3],\n",
    "            \"temperature\": [random.randint(-100,100), random.randint(-100,100), random.randint(-100,100)]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after this rather long day, maybe you can relate a bit more to this:\n",
    "\n",
    "<img src=\"https://memegenerator.net/img/instances/51740682/you-are-a-python-lover-if-pandas-doesnt-mean-but-httppandaspydataorg.jpg\n",
    "\" alt=\"Drawing\" style=\"width: 400px;\"/ align=\"left\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
